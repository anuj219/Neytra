
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>üëì Neytra - Smart Assistant</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      min-height: 100vh;
      padding: 10px;
    }

    .header {
      text-align: center;
      color: white;
      margin-bottom: 20px;
    }

    .header h1 {
      font-size: 2.5em;
      margin-bottom: 5px;
    }

    .header p {
      font-size: 1em;
      opacity: 0.9;
    }

    /* Mode Display */
    .mode-display {
      background: rgba(255, 255, 255, 0.95);
      border-radius: 15px;
      padding: 15px;
      margin-bottom: 15px;
      text-align: center;
      box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    }

    .mode-display h2 {
      color: #667eea;
      font-size: 1.3em;
      margin-bottom: 5px;
    }

    .mode-display .mode-description {
      color: #666;
      font-size: 0.9em;
    }

    /* Video Container */
    .video-container {
      position: relative;
      background: #000;
      border-radius: 15px;
      overflow: hidden;
      margin-bottom: 15px;
      box-shadow: 0 5px 20px rgba(0,0,0,0.3);
    }

    #cam {
      width: 100%;
      display: block;
    }

    .video-overlay {
      position: absolute;
      top: 10px;
      right: 10px;
      background: rgba(0, 0, 0, 0.7);
      padding: 8px 15px;
      border-radius: 20px;
      display: flex;
      align-items: center;
      gap: 8px;
    }

    .status-dot {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: #ff4444;
      animation: pulse 2s infinite;
    }

    .status-dot.active {
      background: #44ff44;
    }

    .video-overlay span {
      color: white;
      font-size: 0.9em;
      font-weight: 600;
    }

    /* Output Display */
    #output {
      background: white;
      border-radius: 15px;
      padding: 15px;
      margin-bottom: 15px;
      min-height: 100px;
      box-shadow: 0 3px 10px rgba(0,0,0,0.2);
    }

    #output strong {
      color: #667eea;
    }

    /* Controls */
    .controls {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 10px;
      margin-bottom: 15px;
    }

    .controls button {
      padding: 15px;
      border: none;
      border-radius: 10px;
      font-size: 1em;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      background: white;
      color: #667eea;
      box-shadow: 0 3px 10px rgba(0,0,0,0.2);
    }

    .controls button:active {
      transform: translateY(2px);
    }

    .controls button.active {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
    }

    /* Voice Section */
    .voice-section {
      background: white;
      border-radius: 15px;
      padding: 20px;
      text-align: center;
      box-shadow: 0 3px 10px rgba(0,0,0,0.2);
    }

    .voice-trigger-label {
      color: #667eea;
      font-weight: 600;
      margin-bottom: 15px;
      font-size: 1.1em;
    }

    #micBtn {
      width: 80px;
      height: 80px;
      border-radius: 50%;
      border: none;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      cursor: pointer;
      transition: all 0.3s ease;
      box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
      display: flex;
      align-items: center;
      justify-content: center;
      margin: 0 auto 15px;
    }

    #micBtn svg {
      width: 35px;
      height: 35px;
      fill: white;
    }

    #micBtn:hover {
      transform: scale(1.05);
    }

    #micBtn:active {
      transform: scale(0.95);
    }

    #micBtn.listening {
      background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
      animation: pulse 1.5s infinite;
    }

    .voice-feedback {
      min-height: 30px;
      color: #666;
      font-size: 0.95em;
    }

    .loading {
      display: inline-block;
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: #667eea;
      animation: pulse 1s infinite;
    }

    @keyframes pulse {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.5; }
    }

    /* Responsive */
    @media (max-width: 600px) {
      .header h1 {
        font-size: 2em;
      }
      
      .controls {
        grid-template-columns: 1fr;
      }
    }
  </style>
</head>
<body>

  <div class="header">
    <h1>üëì Neytra</h1>
    <p>Agentic AI Assistant</p>
  </div>

  <!-- Mode Display -->
  <div class="mode-display">
    <h2 id="currentMode">SCAN MODE</h2>
    <p class="mode-description" id="modeDescription">Standard object detection every 10 seconds</p>
  </div>

  <!-- Video Container -->
  <div class="video-container">
    <video id="cam" autoplay playsinline muted></video>
    <div class="video-overlay">
      <div class="status-dot" id="statusDot"></div>
      <span id="statusText">Disconnected</span>
    </div>
  </div>

  <!-- Output Display -->
  <div id="output">
    <strong>Status:</strong> Waiting for input...
  </div>

  <!-- Control Buttons -->
  <div class="controls">
    <button id="toggleBtn">Start Scan Mode</button>
    <button id="analyzeBtn">Analyze Scene</button>
  </div>

  <!-- Voice Input Section -->
  <div class="voice-section">
    <div class="voice-trigger-label">Press to Give Voice Command</div>
    <button id="micBtn" aria-label="Voice Input">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
        <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z" />
        <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z" />
      </svg>
    </button>
    <div class="voice-feedback" id="voiceFeedback"></div>
  </div>

  <script>
    // =============================================
    // CONFIGURATION
    // =============================================
    const backendURL = "http://192.168.134.214:8000";

    // =============================================
    // STATE MANAGEMENT
    // =============================================
    let streaming = false;
    let intervalId = null;
    let isListening = false;
    let recognition = null;
    let currentMode = 'scan'; // scan, quickscan, face, vision
    let isProcessing = false;

    // =============================================
    // DOM ELEMENTS
    // =============================================
    const video = document.getElementById("cam");
    const output = document.getElementById("output");
    const micBtn = document.getElementById("micBtn");
    const voiceFeedback = document.getElementById("voiceFeedback");
    const statusDot = document.getElementById("statusDot");
    const statusText = document.getElementById("statusText");
    const currentModeDisplay = document.getElementById("currentMode");
    const modeDescription = document.getElementById("modeDescription");
    const toggleBtn = document.getElementById("toggleBtn");
    const analyzeBtn = document.getElementById("analyzeBtn");

    // =============================================
    // MODE CONFIGURATION
    // =============================================
    const MODE_CONFIG = {
      scan: {
        endpoint: '/api/scan',
        interval: 10000, // 10 seconds
        name: 'SCAN MODE',
        description: 'Standard object detection every 10 seconds'
      },
      quickscan: {
        endpoint: '/api/quickscan',
        interval: 2000, // 2 seconds for urgent scenarios
        name: 'QUICK SCAN MODE',
        description: 'Fast detection for urgent scenarios'
      },
      face: {
        endpoint: '/api/face',
        interval: 5000, // 5 seconds
        name: 'FACE RECOGNITION MODE',
        description: 'Identifying people around you'
      },
      vision: {
        endpoint: '/api/vision',
        interval: null, // One-shot only
        name: 'VISION MODE',
        description: 'AI analysis of the scene'
      }
    };

    // =============================================
    // 1. CAMERA INITIALIZATION
    // =============================================
    async function startCamera() {
      try {
        const constraints = {
          video: { facingMode: { ideal: "environment" } }, // Back camera
          audio: false,
        };

        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        updateStatus("Ready", false);
        console.log('[CAMERA] Started successfully');
      } catch (err) {
        output.innerHTML = `<strong>Camera Error:</strong> ${err}`;
        updateStatus("Error", false);
        console.error('[CAMERA ERROR]', err);
      }
    }

    // Initialize camera on load
    startCamera();

    // =============================================
    // 2. FRAME CAPTURE AND SEND
    // =============================================
    async function captureAndSendFrame() {
      if (!streaming || isProcessing) return;

      isProcessing = true;

      try {
        const canvas = document.createElement("canvas");
        canvas.width = 640;
        canvas.height = 480;
        const ctx = canvas.getContext("2d");
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        const blob = await new Promise((resolve) =>
          canvas.toBlob(resolve, "image/jpeg", 0.7)
        );

        const formData = new FormData();
        formData.append("file", blob, "frame.jpg");

        const endpoint = MODE_CONFIG[currentMode].endpoint;
        console.log(`[${currentMode.toUpperCase()}] Sending frame to ${endpoint}`);

        const res = await fetch(`${backendURL}${endpoint}`, {
          method: "POST",
          body: formData
        });

        const data = await res.json();
        console.log(`[${currentMode.toUpperCase()}] Response:, data`);

        handleModeResponse(data);

      } catch (err) {
        output.innerHTML = `<strong>Connection Error:</strong> ${err}`;
        console.error('[FRAME ERROR]', err);
      } finally {
        isProcessing = false;
      }
    }

    // =============================================
    // 3. MODE-SPECIFIC RESPONSE HANDLING
    // =============================================
    function handleModeResponse(data) {
      switch (currentMode) {
        case 'scan':
        case 'quickscan':
          handleObjectDetection(data);
          break;
        case 'face':
          handleFaceRecognition(data);
          break;
        case 'vision':
          handleVisionAnalysis(data);
          break;
      }
    }

    function handleObjectDetection(data) {
      if (!data.detections || data.detections.length === 0) {
        output.innerHTML = "<strong>Status:</strong> No objects detected.";
        return;
      }

      const FRAME_WIDTH = 640;
      const center = FRAME_WIDTH / 2;
      const margin = FRAME_WIDTH * 0.2;

      function getPosition(bbox) {
        if (!bbox || bbox.length < 4) return "ahead";
        const [x1, , x2] = bbox;
        const cx = (x1 + x2) / 2;
        if (cx < center - margin) return "on your left";
        if (cx > center + margin) return "on your right";
        return "ahead";
      }

      let text = `<strong>Detected (${data.mode}):</strong><br>`;
      const phrases = [];

      data.detections.forEach((det) => {
        const pos = getPosition(det.bbox);
        const priority = det.priority ? '‚ö† ' : '';
        const confidence = det.confidence ? `${(det.confidence * 100).toFixed(0)}%` : '';
        
        text += `${priority}üì¶ ${det.label} ${pos} ${confidence}<br>`;
        
        if (det.priority || currentMode === 'quickscan') {
          phrases.push( `${det.label} ${pos}`);
        }
      });

      output.innerHTML = text;

      if (phrases.length > 0) {
        const sentence = phrases.join(", ");
        speak(sentence);
      }
    }

    function handleFaceRecognition(data) {
      let text = "<strong>Face Recognition:</strong><br>";
      const phrases = [];

      if (data.faces && data.faces.length > 0) {
        data.faces.forEach((face) => {
          const icon = face.name === 'unknown' ? '‚ùì' : 'üë§';
          text += `${icon} ${face.name}<br>`;
          
          if (face.announce && face.name !== 'unknown') {
            phrases.push(face.name);
          }
        });
      }

      if (data.objects && data.objects.length > 0) {
        data.objects.forEach((obj) => {
          text += `üì¶ ${obj.label}<br>`;
        });
      }

      if (!data.faces?.length && !data.objects?.length) {
        text += "No faces or objects detected.";
      }

      output.innerHTML = text;

      if (phrases.length > 0) {
        speak(phrases.join(", ") + " detected");
      }
    }

    function handleVisionAnalysis(data) {
      if (data.description) {
        output.innerHTML = `<strong>Vision Analysis:</strong><br>${data.description}`;
        speak(data.description);
      } else {
        output.innerHTML = "<strong>Error:</strong> No analysis received.";
      }
    }

    // =============================================
    // 4. TEXT-TO-SPEECH
    // =============================================
    function speak(text) {
      if ('speechSynthesis' in window) {
        window.speechSynthesis.cancel();
        const msg = new SpeechSynthesisUtterance(text);
        msg.rate = 1.0;
        msg.pitch = 1.0;
        msg.volume = 1.0;
        window.speechSynthesis.speak(msg);
      }
    }

    // =============================================
    // 5. MODE SWITCHING
    // =============================================
    function switchMode(newMode) {
      // Stop current streaming
      if (streaming) {
        stopStreaming();
      }

      // Update mode
      currentMode = newMode;
      const config = MODE_CONFIG[newMode];

      // Update UI
      currentModeDisplay.textContent = config.name;
      modeDescription.textContent = config.description;

      console.log(`[MODE] Switched to ${newMode}`);
    }

    function startStreaming() {
      if (streaming) return;

      const config = MODE_CONFIG[currentMode];
      
      if (!config.interval) {
        // One-shot mode (vision)
        captureAndSendFrame();
        return;
      }

      streaming = true;
      toggleBtn.textContent = `Stop ${config.name}`;
      toggleBtn.classList.add('active');
      updateStatus(`${config.name} Active, true`);

      // Immediate first frame
      captureAndSendFrame();

      // Then periodic updates
      intervalId = setInterval(captureAndSendFrame, config.interval);

      console.log(`[STREAMING] Started in ${currentMode} mode (${config.interval}ms interval)`);
    }

    function stopStreaming() {
      if (!streaming) return;

      streaming = false;
      clearInterval(intervalId);
      intervalId = null;

      toggleBtn.textContent = 'Start Scan Mode';
      toggleBtn.classList.remove('active');
      updateStatus("Paused", false);

      console.log('[STREAMING] Stopped');
    }

    // =============================================
    // 6. BUTTON HANDLERS
    // =============================================
    toggleBtn.onclick = () => {
      if (streaming) {
        stopStreaming();
      } else {
        startStreaming();
      }
    };

    analyzeBtn.onclick = async () => {
      // Stop current streaming
      if (streaming) {
        stopStreaming();
      }

      // Switch to vision mode temporarily
      const previousMode = currentMode;
      currentMode = 'vision';

      output.innerHTML = "<strong>Status:</strong> Analyzing scene...";
      speak("Analyzing scene");
      updateStatus("Analyzing", false);

      await captureAndSendFrame();

      // Return to previous mode
      currentMode = previousMode;
      updateStatus("Ready", false);
    };

    // =============================================
    // 7. SPEECH RECOGNITION
    // =============================================
    function initSpeechRecognition() {
      if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        recognition = new SpeechRecognition();
        recognition.continuous = false;
        recognition.interimResults = false;
        recognition.lang = 'en-US';

        recognition.onstart = () => {
          isListening = true;
          micBtn.classList.add('listening');
          voiceFeedback.innerHTML = '<span class="loading"></span> Listening...';
          console.log('[VOICE] Started listening');
        };

        recognition.onresult = (event) => {
          const transcript = event.results[0][0].transcript;
          console.log(`[VOICE] Recognized: "${transcript}"`);
          voiceFeedback.textContent = `You said: "${transcript}"`;

          // Send to backend for intent detection
          processVoiceCommand(transcript);

          // Auto-clear feedback after 3 seconds
          setTimeout(() => {
            if (voiceFeedback) voiceFeedback.textContent = '';
          }, 3000);
        };

        recognition.onerror = (event) => {
          console.error('[VOICE] Error:', event.error);

          let errorMsg = 'Could not recognize speech';
          if (event.error === 'no-speech') {
            errorMsg = 'No speech detected';
          } else if (event.error === 'network') {
            errorMsg = 'Network error';
          } else if (event.error === 'aborted') {
            errorMsg = 'Recognition aborted';
          }

          voiceFeedback.textContent = `Error: ${errorMsg}`;
          setTimeout(() => {
            if (voiceFeedback) voiceFeedback.textContent = '';
          }, 3000);
        };

        recognition.onend = () => {
          isListening = false;
          micBtn.classList.remove('listening');
          console.log('[VOICE] Stopped listening');
        };

        console.log('[VOICE] Speech recognition initialized');
        return true;
      } else {
        console.warn('[VOICE] Speech recognition not supported');
        if (voiceFeedback) {
          voiceFeedback.textContent = 'Voice input not supported on this browser';
        }
        return false;
      }
    }

    // Initialize speech recognition on load
    initSpeechRecognition();

    // =============================================
    // 8. MICROPHONE BUTTON HANDLER
    // =============================================
    if (micBtn) {
      micBtn.addEventListener('click', () => {
        if (!recognition) {
          voiceFeedback.textContent = 'Voice input not available';
          setTimeout(() => {
            voiceFeedback.textContent = '';
          }, 2000);
          return;
        }

        if (isListening) {
          recognition.stop();
          console.log('[VOICE] Manually stopped');
        } else {
          try {
            recognition.start();
          } catch (error) {
            console.error('[VOICE] Start error:', error);
            // Already running, stop and restart
            recognition.stop();
            setTimeout(() => {
              try {
                recognition.start();
              } catch (e) {
                console.error('[VOICE] Restart failed:', e);
              }
            }, 300);
          }
        }
      });
    }

    // =============================================
    // 9. PROCESS VOICE COMMAND
    // =============================================
    async function processVoiceCommand(command) {
      try {
        console.log('[VOICE] Processing command:', command);
        
        // Show processing state
        voiceFeedback.innerHTML = '<span class="loading"></span> Processing...';

        // Step 1: Send command to backend for intent detection
        const response = await fetch(`${backendURL}/voice-command`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({ command: command })
        });

        if (!response.ok) {
          throw new Error(`Server error: ${response.status}`);
        }

        const modeData = await response.json();
        console.log('[VOICE] Backend response:', modeData);

        // Step 2: Handle the detected mode
        await handleVoiceMode(modeData, command);

        // Clear feedback
        voiceFeedback.textContent = '';

      } catch (err) {
        console.error('[VOICE] Processing error:', err);
        voiceFeedback.textContent = 'Failed to process command';
        speak('Sorry, I could not process that command');
        
        setTimeout(() => {
          voiceFeedback.textContent = '';
        }, 2000);
      }
    }

    // =============================================
    // 10. HANDLE VOICE-DETECTED MODE
    // =============================================
    async function handleVoiceMode(modeData, originalCommand) {
      const { mode, prompt, endpoint } = modeData;

      console.log(`[VOICE] Switching to mode: ${mode}`);

      // Stop current streaming
      if (streaming) {
        stopStreaming();
      }

      switch (mode) {
        case 'scan':
          switchMode('scan');
          startStreaming();
          speak('Scan mode activated');
          break;

        case 'quickscan':
          switchMode('quickscan');
          startStreaming();
          speak('Quick scan mode activated. Watching for obstacles.');
          break;

        case 'face':
          switchMode('face');
          startStreaming();
          speak('Face recognition mode activated');
          break;

        case 'vision':
          // For vision mode, capture frame and send with prompt
          switchMode('vision');
          output.innerHTML = `<strong>Question:</strong> ${originalCommand}<br><strong>Status:</strong> Analyzing...;
          speak('Let me analyze that')`;

          try {
            const canvas = document.createElement("canvas");
            canvas.width = 640;
            canvas.height = 480;
            const ctx = canvas.getContext("2d");
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            const blob = await new Promise((resolve) =>
              canvas.toBlob(resolve, "image/jpeg", 0.8)
            );

            const formData = new FormData();
            formData.append("file", blob, "frame.jpg");
            
            // Add the prompt from LLM
            if (prompt) {
              formData.append("prompt", prompt);
            }

            const response = await fetch(`${backendURL}${endpoint}`, {
              method: 'POST',
              body: formData
            });

            const result = await response.json();
            
            output.innerHTML = `<strong>Question:</strong> ${originalCommand}<br><strong>Answer:</strong> ${result.description};
            speak(result.description)`;

          } catch (err) {
            console.error('[VISION ERROR]', err);
            output.innerHTML = `<strong>Error:</strong> Failed to analyze scene`;
            speak('Sorry, I could not analyze the scene');
          }
          break;

        default:
          console.log('[VOICE] Unknown mode:', mode);
          speak('I did not understand that command');
      }
    }

    // =============================================
    // 11. STATUS INDICATOR UPDATE
    // =============================================
    function updateStatus(text, active) {
      if (statusText) statusText.textContent = text;
      if (statusDot) {
        if (active) {
          statusDot.classList.add('active');
        } else {
          statusDot.classList.remove('active');
        }
      }
    }

    // =============================================
    // 12. KEYBOARD SHORTCUTS (Optional)
    // =============================================
    document.addEventListener('keydown', (e) => {
      // Press 'V' to trigger voice input
      if (e.key === 'v' || e.key === 'V') {
        if (micBtn && !isListening) {
          micBtn.click();
        }
      }

      // Press 'S' to toggle streaming
      if (e.key === 's' || e.key === 'S') {
        toggleBtn.click();
      }

      // Press 'A' to analyze scene
      if (e.key === 'a' || e.key === 'A') {
        analyzeBtn.click();
      }

      // Press '1' for scan mode
      if (e.key === '1') {
        switchMode('scan');
        speak('Scan mode');
      }

      // Press '2' for quickscan mode
      if (e.key === '2') {
        switchMode('quickscan');
        speak('Quick scan mode');
      }

      // Press '3' for face recognition mode
      if (e.key === '3') {
        switchMode('face');
        speak('Face recognition mode');
      }
    });

    // =============================================
    // 13. WAKE LOCK (Prevent Screen Sleep)
    // =============================================
    let wakeLock = null;

    async function requestWakeLock() {
      if ('wakeLock' in navigator) {
        try {
          wakeLock = await navigator.wakeLock.request('screen');
          console.log('[WAKELOCK] Screen wake lock activated');

          wakeLock.addEventListener('release', () => {
            console.log('[WAKELOCK] Screen wake lock released');
          });
        } catch (err) {
          console.error('[WAKELOCK] Error:', err);
        }
      }
    }

    // Request wake lock when streaming starts
    const originalStartStreaming = startStreaming;
    startStreaming = function() {
      originalStartStreaming();
      if (streaming) {
        requestWakeLock();
      }
    };

    const originalStopStreaming = stopStreaming;
    stopStreaming = function() {
      originalStopStreaming();
      if (wakeLock) {
        wakeLock.release();
        wakeLock = null;
      }
    };

    // =============================================
    // 14. PAGE VISIBILITY (Pause when hidden)
    // =============================================
    document.addEventListener('visibilitychange', () => {
      if (document.hidden) {
        console.log('[PAGE] Hidden, pausing...');
        if (streaming) {
          stopStreaming();
        }
      } else {
        console.log('[PAGE] Visible again');
      }
    });

    // =============================================
    // 15. CONNECTION CHECK
    // =============================================
    async function checkServerConnection() {
      try {
        const response = await fetch(`${backendURL}/health`);
        const health = await response.json();
        
        console.log('[SERVER] Health check:', health);
        
        if (!health.yolo_model_loaded) {
          output.innerHTML = '<strong>‚ö† Warning:</strong> YOLO model not loaded. Object detection may not work.';
        }
        
        if (!health.groq_configured) {
          console.warn('[SERVER] Groq API not configured');
        }

        updateStatus("Connected", false);
        return true;

      } catch (err) {
        console.error('[SERVER] Connection failed:', err);
        updateStatus("Server Offline", false);
        output.innerHTML = '<strong>Error:</strong> Cannot connect to server. Please check if the backend is running.';
        return false;
      }
    }

    // Check connection on load
    setTimeout(checkServerConnection, 1000);

    // =============================================
    // 16. PERIODIC CONNECTION CHECK
    // =============================================
    setInterval(() => {
      if (!streaming) {
        checkServerConnection();
      }
    }, 30000); // Check every 30 seconds when not streaming

    // =============================================
    // 17. ERROR RECOVERY
    // =============================================
    window.addEventListener('error', (event) => {
      console.error('[GLOBAL ERROR]', event.error);
    });

    // =============================================
    // 18. INITIALIZATION MESSAGE
    // =============================================
    console.log(`
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë   üëì NEYTRA INITIALIZED               ‚ïë
    ‚ïë   Agentic AI Assistant                ‚ïë
    ‚ïë                                       ‚ïë
    ‚ïë   Modes Available:                    ‚ïë
    ‚ïë   ‚Ä¢ Scan (10s interval)              ‚ïë
    ‚ïë   ‚Ä¢ Quick Scan (2s interval)         ‚ïë
    ‚ïë   ‚Ä¢ Face Recognition (5s interval)   ‚ïë
    ‚ïë   ‚Ä¢ Vision LLM (on-demand)           ‚ïë
    ‚ïë                                       ‚ïë
    ‚ïë   Keyboard Shortcuts:                 ‚ïë
    ‚ïë   V - Voice input                     ‚ïë
    ‚ïë   S - Toggle streaming                ‚ïë
    ‚ïë   A - Analyze scene                   ‚ïë
    ‚ïë   1,2,3 - Switch modes                ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    `);

    // Welcome message
    setTimeout(() => {
      speak('Neytra initialized and ready');
    }, 1500);

  </script>

</body>
</html>

